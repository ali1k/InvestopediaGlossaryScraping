{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (20.2.4)\n",
      "Requirement already satisfied: install in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (from python-slugify) (1.3)\n",
      "Requirement already satisfied: pip in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (20.2.4)\n",
      "Requirement already satisfied: install in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1277 sha256=25e07933f00cccd226db5310650c03a87f232b490433719187c55ce5f86f8359\n",
      "  Stored in directory: c:\\users\\alkhalili\\appdata\\local\\pip\\cache\\wheels\\75\\78\\21\\68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Requirement already satisfied: pip in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (20.2.4)\n",
      "Requirement already satisfied: install in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\alkhalili\\anaconda3\\lib\\site-packages (4.6.1)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install python-slugify\n",
    "!{sys.executable} -m pip install pip install bs4\n",
    "!{sys.executable} -m pip install pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, random, logging, urllib.request, json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.investopedia.com/financial-term-dictionary-4769738'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_links = []\n",
    "\n",
    "page = urllib.request.urlopen(url).read().decode('utf8','ignore') \n",
    "soup = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "for link in soup.find_all('a',{'class': 'terms-bar__link mntl-text-link'},  href = True):\n",
    "\n",
    "    master_links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_links = master_links[0:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('URL_INDEX_BY_ALPHA.txt', 'w') as f:\n",
    "    for item in master_links:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_alpha = []\n",
    "\n",
    "for articleIdx in master_links:\n",
    "    \n",
    "    page = urllib.request.urlopen(articleIdx).read().decode('utf8','ignore') \n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "                         \n",
    "    for link in soup.find_all('a',{'class': 'dictionary-top300-list__list mntl-text-link'},  href = True):\n",
    "           \n",
    "            list_alpha.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FULL_URL_INDEX.txt', 'w') as f:\n",
    "    for item in list_alpha:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logf = open(\"error.log\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6321/6321 [47:22<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# for article in tqdm(random.sample(list_alpha, 10)):\n",
    "data = {} #json file\n",
    "for article in tqdm(list_alpha):\n",
    "    list_related = []\n",
    "    body = []\n",
    "    try:\n",
    "        \n",
    "        page = urllib.request.urlopen(article, timeout = 3).read().decode('utf8','ignore')\n",
    "        soup = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "        myTags = soup.find_all('p', {'class': 'comp mntl-sc-block finance-sc-block-html mntl-sc-block-html'})\n",
    "    \n",
    "        for link in soup.find_all('a',{'class': 'related-terms__title mntl-text-link'},  href = True):\n",
    "            list_related.append(link.get('href'))\n",
    "    \n",
    "        title = slugify(soup.find('title').get_text(strip=True)) + '.json'\n",
    "        data['name'] = soup.find('title').get_text(strip=True)\n",
    "        data['@id'] = article\n",
    "        data['related'] = list_related\n",
    "        post = ''\n",
    "    \n",
    "        for tag in myTags:\n",
    "            # body.append(str(tag.get_text(strip=True).encode('utf8', errors='replace'))) #get text content\n",
    "            body.append(tag.decode_contents()) # get html content\n",
    "\n",
    "        f = 'data/' + title\n",
    "        data['body'] = body\n",
    "\n",
    "        w = open(f, 'w')\n",
    "        w.write(json.dumps(data))\n",
    "        w.close()\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        logf.write(\"Failed to extract: {0}\\n\".format(str(article)))\n",
    "        logging.error(\"Exception occurred\", exc_info=True)\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handle items which are not yet scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "federal-reserve-system-frs-definition.json\n"
     ]
    }
   ],
   "source": [
    "items_remained= ['https://www.investopedia.com/terms/f/federalreservebank.asp']\n",
    "data = {} #json file\n",
    "for article in items_remained:\n",
    "    list_related = []\n",
    "    body = []\n",
    "    try:\n",
    "        \n",
    "        page = urllib.request.urlopen(article, timeout = 3).read().decode('utf8','ignore')\n",
    "        soup = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "        myTags = soup.find_all('p', {'class': 'comp mntl-sc-block finance-sc-block-html mntl-sc-block-html'})\n",
    "    \n",
    "        for link in soup.find_all('a',{'class': 'related-terms__title mntl-text-link'},  href = True):\n",
    "            list_related.append(link.get('href'))\n",
    "    \n",
    "        title = slugify(soup.find('title').get_text(strip=True)) + '.json'\n",
    "        data['name'] = soup.find('title').get_text(strip=True)\n",
    "        print(title)\n",
    "        data['@id'] = article\n",
    "        data['related'] = list_related\n",
    "        post = ''\n",
    "    \n",
    "        for tag in myTags:\n",
    "            # body.append(str(tag.get_text(strip=True).encode('utf8', errors='replace'))) #get text content\n",
    "            body.append(tag.decode_contents()) # get html content\n",
    "\n",
    "        f = 'data/' + title\n",
    "        data['body'] = body\n",
    "\n",
    "        w = open(f, 'w')\n",
    "        w.write(json.dumps(data))\n",
    "        w.close()\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        logf.write(\"Failed to extract: {0}\\n\".format(str(article)))\n",
    "        logging.error(\"Exception occurred\", exc_info=True)\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
