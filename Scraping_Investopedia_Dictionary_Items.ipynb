{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/site-packages (21.0.1)\n",
      "Requirement already satisfied: install in /usr/local/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/site-packages (4.0.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/site-packages (from python-slugify) (1.3)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/site-packages (21.0.1)\n",
      "Requirement already satisfied: install in /usr/local/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/site-packages (21.0.1)\n",
      "Requirement already satisfied: install in /usr/local/lib/python3.8/site-packages (1.3.4)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.3-cp38-cp38-macosx_10_9_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.6.3\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install pip install python-slugify\n",
    "!{sys.executable} -m pip install pip install bs4\n",
    "!{sys.executable} -m pip install pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, random, logging, urllib.request, json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.investopedia.com/financial-term-dictionary-4769738'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_links = []\n",
    "\n",
    "page = urllib.request.urlopen(url).read().decode('utf8','ignore') \n",
    "soup = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "for link in soup.find_all('a',{'class': 'terms-bar__link mntl-text-link'},  href = True):\n",
    "\n",
    "    master_links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_links = master_links[0:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('URL_INDEX_BY_ALPHA.txt', 'w') as f:\n",
    "    for item in master_links:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_alpha = []\n",
    "\n",
    "for articleIdx in master_links:\n",
    "    \n",
    "    page = urllib.request.urlopen(articleIdx).read().decode('utf8','ignore') \n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "                         \n",
    "    for link in soup.find_all('a',{'class': 'dictionary-top300-list__list mntl-text-link'},  href = True):\n",
    "           \n",
    "            list_alpha.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FULL_URL_INDEX.txt', 'w') as f:\n",
    "    for item in list_alpha:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logf = open(\"error.log\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6321/6321 [47:22<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# for article in tqdm(random.sample(list_alpha, 10)):\n",
    "data = {} #json file\n",
    "for article in tqdm(list_alpha):\n",
    "    list_related = []\n",
    "    body = []\n",
    "    try:\n",
    "        \n",
    "        page = urllib.request.urlopen(article, timeout = 3).read().decode('utf8','ignore')\n",
    "        soup = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "        myTags = soup.find_all('p', {'class': 'comp mntl-sc-block finance-sc-block-html mntl-sc-block-html'})\n",
    "    \n",
    "        for link in soup.find_all('a',{'class': 'related-terms__title mntl-text-link'},  href = True):\n",
    "            list_related.append(link.get('href'))\n",
    "    \n",
    "        title = slugify(soup.find('title').get_text(strip=True)) + '.json'\n",
    "        data['name'] = soup.find('title').get_text(strip=True)\n",
    "        data['@id'] = article\n",
    "        data['related'] = list_related\n",
    "        post = ''\n",
    "    \n",
    "        for tag in myTags:\n",
    "            # body.append(str(tag.get_text(strip=True).encode('utf8', errors='replace'))) #get text content\n",
    "            body.append(tag.decode_contents()) # get html content\n",
    "\n",
    "        f = 'data/' + title\n",
    "        data['body'] = body\n",
    "\n",
    "        w = open(f, 'w')\n",
    "        w.write(json.dumps(data))\n",
    "        w.close()\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        logf.write(\"Failed to extract: {0}\\n\".format(str(article)))\n",
    "        logging.error(\"Exception occurred\", exc_info=True)\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handle items which are not yet scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_remained= ['https://www.investopedia.com/terms/1/529plan.asp', 'https://www.investopedia.com/terms/a/asset.asp', 'https://www.investopedia.com/terms/f/fomc.asp', 'https://www.investopedia.com/terms/f/federalreservebank.asp', 'https://www.investopedia.com/terms/n/no-loadfund.asp', 'https://www.investopedia.com/terms/n/nsf.asp', 'https://www.investopedia.com/terms/p/parvalue.asp', 'https://www.investopedia.com/terms/p/priceelasticity.asp', 'https://www.investopedia.com/terms/s/stocksymbol.asp', 'https://www.investopedia.com/terms/e/expectationstheory.asp', 'https://www.investopedia.com/terms/u/utilities_sector.asp', 'https://www.investopedia.com/terms/v/volatility.asp', 'https://www.investopedia.com/terms/e/elliottwavetheory.asp', 'https://www.investopedia.com/terms/b/bank-reserve.asp', 'https://www.investopedia.com/terms/w/windstorm-insurance.asp', 'https://www.investopedia.com/terms/y/yankeecd.asp']\n",
    "data = {} #json file\n",
    "for article in items_remained:\n",
    "    list_related = []\n",
    "    body = []\n",
    "    try:\n",
    "        \n",
    "        page = urllib.request.urlopen(article, timeout = 3).read().decode('utf8','ignore')\n",
    "        soup = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "        myTags = soup.find_all('p', {'class': 'comp mntl-sc-block finance-sc-block-html mntl-sc-block-html'})\n",
    "    \n",
    "        for link in soup.find_all('a',{'class': 'related-terms__title mntl-text-link'},  href = True):\n",
    "            list_related.append(link.get('href'))\n",
    "    \n",
    "        title = slugify(soup.find('title').get_text(strip=True)) + '.json'\n",
    "        data['name'] = soup.find('title').get_text(strip=True)\n",
    "        print(data['name'])\n",
    "        data['@id'] = article\n",
    "        data['related'] = list_related\n",
    "        post = ''\n",
    "    \n",
    "        for tag in myTags:\n",
    "            # body.append(str(tag.get_text(strip=True).encode('utf8', errors='replace'))) #get text content\n",
    "            body.append(tag.decode_contents()) # get html content\n",
    "\n",
    "        f = 'data/' + title\n",
    "        data['body'] = body\n",
    "\n",
    "        w = open(f, 'w')\n",
    "        w.write(json.dumps(data))\n",
    "        w.close()\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        logf.write(\"Failed to extract: {0}\\n\".format(str(article)))\n",
    "        logging.error(\"Exception occurred\", exc_info=True)\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
